{"pathArr":[],"metaData":{"title":"Writing an Interpreter in Go"},"content":"\nThorsten Ball\n\n## Introduction\n\n인터프리터는 매우 다양한 종류가 있지만, 근본적으로는 모두 소스 코드를 받아 평가하여 실행될 수 있는 눈에 보이는 중간 산물을 만든다는 것이다.\n\n이 책에서는 소스 코드를 파싱해서 AST를 만들고 이를 평가하는 **tree walking interpreter**를 만들 것이다.\n\n### The Monkey Programming Language & Interpreter\n\n아래의 특징을 가지고\n\n- C-like syntax\n- variable bindings\n- integers and booleans\n- arithmetic expressions\n- built-in functions\n- first-class and higher-order functions\n- closures\n- a string data structure\n- an array data structure\n- a hash data structure\n\n아래 부분들로 구성된다.\n\n- the lexer\n- the parser\n- the Abstract Syntax Tree (AST) • the internal object system\n- the evaluator\n\n## 1. Lexing\n\n### 1. Lexical Analysis\n\n소스 코드를 다루기 쉬운 다른 형태로 바꿀 필요성이 있다. 소스 코드에서 토큰으로, 토큰에서 AST로.\n\n토큰으로의 변환은 **lexer**(tokenizer, scanner)가 맡는다.\n\nAST로의 변환은 **parser**가 맡는다.\n\n### 2. Defining Our Tokens\n\n```monkey\n// lex할 첫번째 Monkey Language\nlet five = 5; let ten = 10;\nlet add = fn(x, y) {\n  x + y;\n};\nlet result = add(five, ten);\n```\n\nnumber, identifier, keyword로 구성된다.\n\n```go\npackage token\n\ntype TokenType string\n\ntype Token struct {\n\tType    TokenType\n\tLiteral string\n}\n\nconst (\n\t// 우리가 모르는 토큰/문자\n\tILLEGAL = \"ILLEGAL\"\n\tEOF     = \"EOF\"\n\n\t// Identifiers + literals\n\tIDENT = \"IDENT\"\n\tINT   = \"INT\"\n\n\t// Operators\n\tASSIGN   = \"=\"\n\tPLUS     = \"+\"\n\tMINUS    = \"-\"\n\tBANG     = \"!\"\n\tASTERISK = \"*\"\n\tSLASH    = \"/\"\n\tLT       = \"<\"\n\tGT       = \">\"\n\tEQ       = \"==\"\n\tNOT_EQ   = \"!=\"\n\n\t// Delimiters\n\tCOMMA     = \",\"\n\tSEMICOLON = \";\"\n\tLPAREN    = \"(\"\n\tRPAREN    = \")\"\n\tLBRACE    = \"{\"\n\tRBRACE    = \"}\"\n\n\t// Keywords\n\tFUNCTION = \"FUNCTION\"\n\tLET      = \"LET\"\n\tTRUE     = \"TRUE\"\n\tFALSE    = \"FALSE\"\n\tIF       = \"IF\"\n\tELSE     = \"ELSE\"\n\tRETURN   = \"RETURN\"\n)\n\nvar keywords = map[string]TokenType{\n\t\"fn\":     FUNCTION,\n\t\"let\":    LET,\n\t\"true\":   TRUE,\n\t\"false\":  FALSE,\n\t\"if\":     IF,\n\t\"else\":   ELSE,\n\t\"return\": RETURN,\n}\n\nfunc LookupIdent(ident string) TokenType {\n\tif tok, ok := keywords[ident]; ok {\n\t\treturn tok\n\t}\n\treturn IDENT\n}\n```\n\n### 3. The Lexer\n\n> We’ll initialize the lexer with our source code and then repeatedly call NextToken() on it to go through the source code, token by token, character by character.\n\n```go\npackage lexer\n\nimport \"monkey/token\"\n\ntype Lexer struct {\n\tinput        string\n\tposition     int\n\treadPosition int\n\tch           byte\n}\n\nfunc New(input string) *Lexer {\n\tl := &Lexer{input: input}\n\tl.readChar()\n\treturn l\n}\n\nfunc (l *Lexer) readChar() {\n\tif l.readPosition >= len(l.input) {\n\t\tl.ch = 0\n\t} else {\n\t\tl.ch = l.input[l.readPosition]\n\t}\n\tl.position = l.readPosition\n\tl.readPosition += 1\n}\n\nfunc (l *Lexer) NextToken() token.Token {\n\tvar tok token.Token\n\tl.skipWhitespace()\n\n\tswitch l.ch {\n\tcase '=':\n\t\tif l.peekChar() == '=' {\n\t\t\tch := l.ch\n\t\t\tl.readChar()\n\t\t\t// 덧셈 왜하지 그냥 \"==\"쓰면 안되나\n\t\t\t// 일반화할 수 있다는걸 보여주려고 그런가?\n\t\t\ttok = token.Token{Type: token.EQ, Literal: string(ch) + string(l.ch)}\n\t\t} else {\n\t\t\ttok = newToken(token.ASSIGN, l.ch)\n\t\t}\n\tcase '+':\n\t\ttok = newToken(token.PLUS, l.ch)\n\tcase '-':\n\t\ttok = newToken(token.MINUS, l.ch)\n\tcase '!':\n\t\tif l.peekChar() == '=' {\n\t\t\tch := l.ch\n\t\t\tl.readChar()\n\t\t\ttok = token.Token{Type: token.NOT_EQ, Literal: string(ch) + string(l.ch)}\n\t\t} else {\n\t\t\ttok = newToken(token.BANG, l.ch)\n\t\t}\n\tcase '/':\n\t\ttok = newToken(token.SLASH, l.ch)\n\tcase '*':\n\t\ttok = newToken(token.ASTERISK, l.ch)\n\tcase '<':\n\t\ttok = newToken(token.LT, l.ch)\n\tcase '>':\n\t\ttok = newToken(token.GT, l.ch)\n\tcase ';':\n\t\ttok = newToken(token.SEMICOLON, l.ch)\n\tcase ',':\n\t\ttok = newToken(token.COMMA, l.ch)\n\tcase '(':\n\t\ttok = newToken(token.LPAREN, l.ch)\n\tcase ')':\n\t\ttok = newToken(token.RPAREN, l.ch)\n\tcase '{':\n\t\ttok = newToken(token.LBRACE, l.ch)\n\tcase '}':\n\t\ttok = newToken(token.RBRACE, l.ch)\n\tcase 0:\n\t\ttok.Literal = \"\"\n\t\ttok.Type = token.EOF\n\tdefault:\n\t\tif isLetter(l.ch) {\n\t\t\ttok.Literal = l.readIdentifier()\n\t\t\ttok.Type = token.LookupIdent(tok.Literal)\n\t\t\treturn tok\n\t\t} else if isDigit(l.ch) {\n\t\t\ttok.Literal = l.readNumber()\n\t\t\ttok.Type = token.INT\n\t\t\treturn tok\n\t\t} else {\n\t\t\ttok = newToken(token.ILLEGAL, l.ch)\n\t\t}\n\t}\n\tl.readChar()\n\treturn tok\n}\n\nfunc newToken(tokenType token.TokenType, ch byte) token.Token {\n\treturn token.Token{Type: tokenType, Literal: string(ch)}\n}\n\nfunc (l *Lexer) readIdentifier() string {\n\tposition := l.position\n\tfor isLetter(l.ch) {\n\t\tl.readChar()\n\t}\n\treturn l.input[position:l.position]\n}\n\nfunc isLetter(ch byte) bool {\n\treturn 'a' <= ch && ch <= 'z' || 'A' <= ch && ch <= 'Z' || ch == '_'\n}\n\n// 종종 eatWhitespace나 consumeWhitespace등으로 불린다.\nfunc (l *Lexer) skipWhitespace() {\n\tfor l.ch == ' ' || l.ch == '\\t' || l.ch == '\\n' || l.ch == '\\r' {\n\t\tl.readChar()\n\t}\n}\n\nfunc (l *Lexer) readNumber() string {\n\tposition := l.position\n\tfor isDigit(l.ch) {\n\t\tl.readChar()\n\t}\n\treturn l.input[position:l.position]\n}\n\nfunc isDigit(ch byte) bool {\n\treturn '0' <= ch && ch <= '9'\n}\n\nfunc (l *Lexer) peekChar() byte {\n\tif l.readPosition >= len(l.input) {\n\t\treturn 0\n\t} else {\n\t\treturn l.input[l.readPosition]\n\t}\n}\n```\n\n> it’s left as an exercise to the reader to fully support Unicode (and emojis!) in Monkey.\n\n### 4. Extending our Token Set and Lexer\n\n따로 소스코드 파일을 만들지는 않음. 위 코드들은 여기 챕터에서 추가된 기능을 담고 있음.\n\n코드가 말이 되고 에러가 있는지 없는지 판단하는 것은 lexer의 일이 아니다.\n\n> The test cases I write for lexers cover all tokens and also try to provoke off-by-one errors, edge cases at end-of-file, newline handling, multi-digit number parsing and so on.\n\n[Off-by-one error](https://en.wikipedia.org/wiki/Off-by-one_error)\n\nMost lexers and parser have such a “peek” function that looks ahead and most of the time it only returns the immediately next character. The difficulty of parsing different languages often comes down to how far you have to peek ahead (or look backwards!) in the source code to make sense of it.\n\n### 5. Start of a REPL\n\nSometimes the REPL is called “console”, sometimes “interactive mode”.\n\n```go\npackage repl\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"io\"\n\t\"monkey/lexer\"\n\t\"monkey/token\"\n)\n\nconst PROMPT = \">>\"\n\nfunc Start(in io.Reader, out io.Writer) {\n\tscanner := bufio.NewScanner(in)\n\n\tfor {\n\t\tfmt.Printf(PROMPT)\n\t\tscanned := scanner.Scan()\n\t\tif !scanned {\n\t\t\treturn\n\t\t}\n\t\tline := scanner.Text()\n\t\tl := lexer.New(line)\n\n\t\tfor tok := l.NextToken(); tok.Type != token.EOF; tok = l.NextToken() {\n\t\t\tfmt.Printf(\"%+v\\n\", tok)\n\t\t}\n\t}\n}\n```\n\n## 2. Parsing\n\n### 1. Parsers\n\n> A parser is a software component that takes input data (frequently text) and builds a data structure – often some kind of parse tree, abstract syntax tree or other hierarchical structure – giving a structural representation of the input, checking for correct syntax in the process. [...] The parser is often preceded by a separate lexical analyser, which creates tokens from the sequence of input characters;\n>\n> Wikipedia\n\nJS의 JSON.parse를 떠올리자. Serialization language의 파서라서 프로그래밍 언어의 파서와는 느낌이 다를 수는 있다.\n\nLisp에서는 소스 코드를 나타내기 위한 데이터 구조를 프로그램에서 데이터로 사용할 수 있다. 'Code is data, data is code\"\n\nAST에는 공백문자, 괄호, 대괄호 등등이 생략되어있을 수 있다.\n\n### 2. Why Not a Parser Generator?\n\nParser generator: yacc, bison, ANTLR, 언어를 묘사하면 parser를 만들어준다. 대부분은 BNF(Backus-Naur Form), EBNF(Extended Backus-Naur Form)과 같은 CFG(context-free grammer)를 입력으로 받는다.\n\n파서는 자동적으로 생성되기에 매우 적합하며 CS에서 잘 연구된 분야이다. \n\n그래도 학습을 위해 스스로 만들어보자. \n\n### 3. Writing a Parser For the Monkey Programming Language\n\ntop-down parsing - recursive descent parsing, early parsing, predictive parsing,,,\nbottom-up parsing\n\nRecursive descent parsing이 직관에 가장 맞는다. \n\n### 4. Parser's First Steps: Parsing Let Statements\n\nPrograms in Monkey are a series of statements.\n\nExpressions produce values, statements don’t.\n\n```go\npackage ast\n\nimport \"monkey/token\"\n\ntype Node interface {\n\t// TokenLiteral() will be used only for debugging and testing.\n\tTokenLiteral() string\n}\n\ntype Statement interface {\n\tNode\n\t// dummy method\n\tstatementNode()\n}\n\ntype Expression interface {\n\tNode\n\t// dummy method\n\texpressionNode()\n}\n\ntype Program struct {\n\tStatements []Statement\n}\n\n// This Program node is going to be the root node of every AST our parser produces.\nfunc (p *Program) TokenLiteral() string {\n\tif len(p.Statements) > 0 {\n\t\treturn p.Statements[0].TokenLiteral()\n\t} else {\n\t\treturn \"\"\n\t}\n}\n\ntype LetStatement struct {\n\tToken token.Token\n\tName  *Identifier\n\tValue Expression\n}\n\nfunc (ls *LetStatement) statementNode()       {}\nfunc (ls *LetStatement) TokenLiteral() string { return ls.Token.Literal }\n\ntype Identifier struct {\n\tToken token.Token\n\tValue string\n}\n\nfunc (i *Identifier) expressionNode()      {}\nfunc (i *Identifier) TokenLiteral() string { return i.Token.Literal }\n```\n\n```go\npackage parser\n\nimport (\n\t\"monkey/ast\"\n\t\"monkey/lexer\"\n\t\"monkey/token\"\n)\n\ntype Parser struct {\n\tl *lexer.Lexer\n\n\tcurToken  token.Token\n\tpeekToken token.Token\n}\n\nfunc New(l *lexer.Lexer) *Parser {\n\tp := &Parser{l: l}\n\n\tp.nextToken()\n\tp.nextToken()\n\n\treturn p\n}\n\nfunc (p *Parser) nextToken() {\n\tp.curToken = p.peekToken\n\tp.peekToken = p.l.NextToken()\n}\n\nfunc (p *Parser) ParseProgram() *ast.Program {\n\treturn nil\n}\n```\n\nWe will get to this later and look at expression parsing in detail, since it’s probably the most complicated but also the most beautiful part of the parser, making heavy use of “Pratt parsing”.\n\nParser는 토큰을 계속 advance하며 현재 토큰을 통해 무슨 작업을 할지 결정한다. \n\n## 3. Evaluation\n\n## 4. Extending the Interpreter\n","path":"cs/writing-an-interpreter-in-go/index.md"}